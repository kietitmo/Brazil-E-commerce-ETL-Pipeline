# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

version: '3.9'
x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: ./airflow/Dockerfile
  environment:
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres_airflow:5432/airflow
    - AIRFLOW__WEBSERVER_BASE_URL=http://localhost:8080
    - AIRFLOW__WEBSERVER__SECRET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
  volumes:
    - ./dags:/opt/airflow/dags
    - ./jobs:/opt/airflow/jobs
    - ./airflow-data/logs:/opt/airflow/logs
    - ./airflow-data/plugins:/opt/airflow/plugins
  depends_on:
    - postgres_airflow
  networks:
    - hive


x-spark-common: &spark-common
  image: bitnami/spark:latest
  volumes:
    - ./jobs:/opt/bitnami/spark/jobs
    - ./hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
  networks:
    - hive
services:
  spark-master:
    hostname: spark-master
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "9090:8080"
      - "7077:7077"

  spark-worker-1:
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1g
      SPARK_MASTER_URL: spark://spark-master:7077

  spark-worker-2:
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1g
      SPARK_MASTER_URL: spark://spark-master:7077

  postgres:
    image: postgres
    restart: unless-stopped
    container_name: postgres
    hostname: postgres
    environment:
      POSTGRES_DB: 'metastore_db'
      POSTGRES_USER: 'hive'
      POSTGRES_PASSWORD: 'password'
    ports:
      - '5432:5432'
    volumes:
      - hive-db:/var/lib/postgresql
    networks:
      - hive

  metastore:
    image: apache/hive:4.0.0
    depends_on:
      - postgres
      - namenode
    restart: unless-stopped
    container_name: metastore
    hostname: metastore
    environment:
      DB_DRIVER: postgres
      SERVICE_NAME: 'metastore'
      SERVICE_OPTS: '-Xmx1G -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
                     -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db
                     -Djavax.jdo.option.ConnectionUserName=hive
                     -Djavax.jdo.option.ConnectionPassword=password'
    ports:
        - '9083:9083'
    volumes:
        - ./hive-site.xml:/opt/hive/conf/hive-site.xml
        - type: bind
          source: ./utils/postgresql/42.5.1/postgresql-42.5.1.jar
          target: /opt/hive/lib/postgres.jar
    networks:
      - hive

  hiveserver2:
    image: apache/hive:4.0.0
    depends_on:
      - metastore
      - namenode
    restart: unless-stopped
    container_name: hiveserver2
    environment:
      HIVE_SERVER2_THRIFT_PORT: 10000
      SERVICE_OPTS: '-Xmx1G -Dhive.metastore.uris=thrift://metastore:9083
                     -Dfs.defaultFS=hdfs://namenode:8020'
      IS_RESUME: 'true'
      SERVICE_NAME: 'hiveserver2'
    ports:
      - '10000:10000'
      - '10002:10002'
    volumes:
      - ./hive-site.xml:/opt/hive/conf/hive-site.xml
    networks:
      - hive

  namenode:
    image: apache/hadoop:3.3.5
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - 9870:9870
    env_file:
      - ./hdfs_config
    environment:
        ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
    networks:
      - hive

  datanode:
    image: apache/hadoop:3.3.5
    command: ["hdfs", "datanode"]
    env_file:
      - ./hdfs_config      
    networks:
      - hive

  resourcemanager:
    image: apache/hadoop:3.3.5
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
        - 8088:8088
    env_file:
      - ./hdfs_config
    volumes:
      - ./test.sh:/opt/test.sh
    networks:
      - hive

  nodemanager:
    image: apache/hadoop:3.3.5
    command: ["yarn", "nodemanager"]
    env_file:
      - ./hdfs_config
    networks:
      - hive

  mysql:
    image: mysql:8.0
    container_name: mysql
    ports:
      - "3306:3306"
    volumes:
      - ./data/initial_query:/docker-entrypoint-initdb.d
      - ./data/csv_files:/var/lib/mysql-files
    environment:
      MYSQL_ROOT_PASSWORD: rootpassword
      MYSQL_DATABASE: mydatabase
      MYSQL_USER: myuser
      MYSQL_PASSWORD: mypassword
    networks:
    - hive

  postgres_airflow:
    hostname: postgres_airflow
    image: postgres:14
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
      - POSTGRES_PORT=5432
    ports:
      - "5433:5432"
    networks:
      - hive

  airflow-init:
    << : *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - airflow db migrate && airflow users create --username airflow --firstname airflow --lastname airflow --role Admin --email airflow@airflow.com --password airflow
    restart: on-failure

  airflow-webserver:
    << : *airflow-common
    command: airflow webserver
    ports:
      - 8080:8080
    restart: always

  airflow-scheduler:
    << : *airflow-common
    command: airflow scheduler
    restart: always

volumes:
  hive-db:
  warehouse:

networks:
  hive:
    name: hive